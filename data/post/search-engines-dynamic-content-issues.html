<p>The problem of taming search engines such as Googlebot, Slurp, MSNbot, Teoma, Gigabot and so on appears frequently when you have a large and dynamic site. There are some sections that can have approximately the same content, like the situation when you are filtering some results in a grid. Many of the filters are in generaly made with forms, but if you have <em>link based</em> filters they can be easely indexed by web crawlers and your rank could decrease. The main problem is that you will serve the robots a lot of pages with no relevancy at all, and the parent pages that point to them will get little credit in the web results.</p> <p><b style="font-size: 16px;">Bad examples </b></p> <p>I'll skip the chit-chat to examples.</p> <table cellspacing="0" cellpadding="2" border="1">   <tbody><tr>     <td bgcolor="#cccccc" colspan="5"><b>Grid example with multiple link based filters </b></td>   </tr>   <tr>     <td colspan="5">Brand filter: <a href="#">Brand 1</a> / <a href="#">Brand 2</a> / <a href="#">Brand 3</a> / <a href="#">Brand 4</a> [ ... ] / <a href="#">Brand 10</a></td>   </tr>   <tr>     <td colspan="5">Price filter: <a href="#">0-50</a> / <a href="#">50 - 150</a> / <a href="#">150 - 300</a> / <a href="#">300 - 550</a> / <a href="#">&gt; 550</a></td>   </tr>   <tr>     <td bgcolor="#eeeeee">Item code (<a href="#">asc</a> / <a href="#">desc</a>)</td>     <td bgcolor="#eeeeee">Item name (<a href="#">asc</a> / <a href="#">desc</a>)</td>     <td bgcolor="#eeeeee">Item price (<a href="#">asc</a> / <a href="#">desc</a>)</td>     <td bgcolor="#eeeeee">Item warranty (<a href="#">asc</a> / <a href="#">desc</a>)</td>     <td bgcolor="#eeeeee">Action</td>   </tr>   <tr>     <td>Code #1</td>     <td>Item #1</td>     <td>Price item #1</td>     <td>Waranty item #1</td>     <td>Add to cart #1</td>   </tr>   <tr>     <td>Code #2</td>     <td>Item #2</td>     <td>Price item #2</td>     <td>Waranty item #2</td>     <td>Add to cart #2</td>   </tr>   <tr>     <td>Code #3</td>     <td>Item #3</td>     <td>Price item #3</td>     <td>Waranty item #3</td>     <td>Add to cart #3</td>   </tr>   <tr>     <td colspan="5">[ ... ]</td>   </tr>   <tr>     <td>Code #20</td>     <td>Item #20</td>     <td>Price item #20</td>     <td>Waranty item #20</td>     <td>Add to cart #20</td>   </tr>   <tr>     <td colspan="5">Page <a href="#">1</a> <a href="#">2</a> <a href="#">3</a> .... <a href="#">100 </a></td>   </tr> </tbody></table> <p>&nbsp;</p><table cellspacing="0" cellpadding="0" border="1">      <tbody><tr>     <td bgcolor="#cccccc" colspan="2"><b>List with examples of URLs from the grid (with filters applied) </b></td>   </tr>   <tr>     <td colspan="2">We suppose that looking at this grid you have to be in an item category section: <b>http://www.site.com/category1</b></td>   </tr>   <tr>     <td>Brand URL:</td>     <td>http://www.site.com/category1/brand1</td>   </tr>   <tr>     <td rowspan="4">Price filter URL:</td>     <td>http://www.site.com/category1/filter/0-50</td>   </tr>   <tr>     <td>http://www.site.com/category1/filter/0-50/2</td>   </tr>   <tr>     <td>http://www.site.com/category1/brand1/filter/0-50</td>   </tr>   <tr>     <td>.. and other combinations</td>   </tr>   <tr>     <td rowspan="4">Sorting URL:</td>     <td>http://www.site.com/category1/brand1/sort=item_code/desc</td>   </tr>   <tr>     <td>http://www.site.com/category1/sort=item_price/asc/2</td>   </tr>   <tr>     <td>http://www.site.com/category1/filter/0-50/sort=item_price/desc/2</td>   </tr>   <tr>     <td>... and other combinations</td>   </tr> </tbody></table> <p>The above example is an actual example of grid of products (items) very often found in online ecommerce shops.</p> <p>A major error for a webmaster would be to allow access of search engines (crawlers) in all the sections of this grid, like the price filter, price filter combinations (eg. brand selected + price filter selected), sorting links or any other combination of these two. Why? Apart of my personal test that i will reveal here, let's take a look at what Google says:</p> <p>&quot;<i>Sites with more content can have more opportunities to rank well in Google. It   makes sense that having more pages of good content represent more chances to   rank in search engine result pages (SERPs). Some SEOs however, do not focus on   the user&rsquo;s needs, but instead create pages solely for search engines. This   approach is based on the<b> false assumption that increasing the volume of web   pages with random, irrelevant content is a good long-term strategy for a site</b>.</i>&quot; (<a href="http://googlewebmastercentral.blogspot.com/2007/03/site-content-and-use-of-web-catalogues.html">Google Webmaster Central Blog post</a>)</p> <p>&quot;Hmm, but my pages in the grid help user navigate on the site!&quot; you might say. True, but they <strong>hurt your rankings</strong> and positioning <strong>if indexed</strong>:</p> <ul>   <li>Search engines  spend a lot of time indexing these pages on your site rather than concentrating on feeding on the parent pages</li>   <li>The content is dynamic, so are the filters. If you reduce the number of items, in our case some pages in the 'Price filters' might dissapear and return 404 or 301. A site that has great index fluctuation will never get appropiate ranks (mainly because this is how spam sites behave).</li>   <li>Using JavaScript (AJAX) is a solution, but not every visitor's browser has JavaScript support, so you need a JavaScript/HTML switch  solution (eg. href=&quot;REAL LINK&quot; onClick=&quot;AJAX LINK&quot;)</li> </ul> <p>You see, the<b> http://www.site.com/category1/brand1/sort=item_code/desc</b> page might contain the <i>same blocks of code</i> also located in <b>http://www.site.com/category1/brand1/3 </b>(meaning page three from items located in Category1 &amp; Brand1). And it's better to focus the search engines on indexing the second URL rather then the first (or the ones with filter applied).</p> <p>Feeding filtered content from your site to search engines is similar to giving  them access to your latest searches on your site. Searching the items in the database of your site is another form of filter.</p> <p>Lots of webmasters optimize the SEO code of the search section of their site just to get more pages in the index. This is a <strong>major mistake</strong> and often goes to having your site not ranked like it is suppose to be or banned from searches. An obvious example is this one:</p> <ul>   <li>URL of the search page: <b>http://www.site.com/search/[<i>keyword</i>]</b></li>   <li>There is a section in the site that prints a list with the latest searches on site, with links that point to the URLs like the one above.</li>   <li>The search engines index the pages and might be stuck there for a while, because you can generate a lot of pages in the search pages (like the filters in the grid). eg. http://www.site.com/search/keyword/filter/0-50 , http://www.site.com/search/keyword/brand1, etc.</li>   <li>Basicly you can generate a ton of URLs from your search section of the site, which is very bad</li> </ul> <p><b style="font-size: 16px;">Solutions</b></p> <p>I gave you some bad examples on how to screw your rankings, now let's see the options we have.</p> <p>In the first example with the grid of items, you can do the following:</p> <ul>   <li>use JavaScript for filters, but you will lose visitors that don't have JavaScript enabled support. If you created the filters entirely with JavaScript and no HTML switch you will lose the rest of the pages from the pagination from getting indexed.</li>   <li>use forms for filters that post information about filtering parameters. You might get into trouble with the code here, because the pagination must obey the filters. The pagination is GET based, and it is recommended to stay that way.</li>   <li>you can use links (hrefs) on all filters with rel=&quot;nofollow&quot; on them. <a href="http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html">Google Official blog post</a>: &quot;<i>From now on, when Google sees the attribute (rel=&quot;nofollow&quot;) on hyperlinks,   those links <b>won't get any credit when we rank websites</b> in our search results.</i>&quot; So this will only resolve half of our problem! That pages will not get credit in the ranking process, but they will still be accesed by the robots: Google Official blog post on<a href="http://googleblog.blogspot.com/2007/02/robots-exclusion-protocol.html"> robots exclusion procedures</a>: &quot;<i>Using NOFOLLOW is generally not the best method to ensure content does not appear in our search results. Using the NOINDEX tag on individual pages or controlling access using robots.txt is the best way to achieve this.</i> &quot;. This means that if we put rel=&quot;nofollow&quot; attribute on &lt;a&gt; tags in the grid, but someone posts a link on their website with an URL like <b>http://www.site.com/category1/brand1/sort=item_code/desc</b>, it will still get indexed. This is also valid for this link appearing in the sitemap with no nofollow attribute on the link.</li> </ul> <p>Best method is to use <b>rel=&quot;nofollow&quot;</b> along with <b>robots.txt</b> directives:</p> <p>User-agent: *<br />   Disallow: /*filter<br /> Disallow: /*sort=</p> <p>Both Googlebot and Slurp claim to recognize this syntax.   See <a href="http://help.yahoo.com/help/us/ysearch/slurp/slurp-02.html">Slurp robots.txt</a> and<a href="http://www.google.com/support/webmasters/bin/answer.py?answer=40367&amp;ctx=sibling"> Googlebot robots.txt</a> info.</p> <p><a name="problems"><b style="font-size: 14px;">Problems</b></a></p> <p>Here are some issues with the web robots that i have encountered:</p> <p>User-agent: * <br /> Disallow: /*pdf$ <br /> Disallow: /*xls$</p> <p>These 2 lines prevent the bots from accesing urls like:<br />   <br />  /accessories/apc/full/xls<br />  /dell/1/ASC/DESC/NONE/xls<br /> /memorii/mediaplayere/prestigio/1/pdf</p> <p>Still MSNbot and Teoma (Ask.com) are not obeying that expression (today is March, 23 2007). Ask.com team says they are looking into it, i don't have an official answer by now. Note that if we remove the $ from Disallow: /*pdf , this line will ban every URL that contain the word 'pdf' (eg. /news/12/we_have_a_new_pdf_catalogue.aspx).</p> <p>MSNbot ignores <i>Disallow: /promo/*/similar_products</i> and indexes <i>http://www.site.com/promo/item_name~95025/similar_products/4</i> . I don't see the bot indexing <i>/promo/item_name~95025/similar_products</i> which is the root page (with pagination).</p> <p>These problems might be generated because of a robots.txt that is updated frequently. If we look at some statistics:</p> <table cellspacing="0" cellpadding="3" border="1">   <tbody><tr>     <td bgcolor="#cccccc" colspan="3"><b>11 March</b> - <b>25 March</b> - robots.txt hits</td>   </tr>   <tr>     <td bgcolor="#eeeeee" colspan="3">website has 150.000+ unique links, age 1 year</td>   </tr>   <tr>     <td bgcolor="#eeeeee">UA</td>     <td bgcolor="#eeeeee">Hits</td>     <td bgcolor="#eeeeee">Last access</td>   </tr>   <tr>     <td>Slurp</td>     <td>1834</td>     <td>2007-03-25 01:46:13</td>   </tr>   <tr>     <td>GoogleBot</td>     <td>1478</td>     <td>2007-03-25 02:25:36</td>   </tr>   <tr>     <td>MSNbot</td>     <td>794</td>     <td>2007-03-25 02:14:31</td>   </tr>   <tr>     <td>Gigabot</td>     <td>120</td>     <td>2007-03-25 01:59:19</td>   </tr>   <tr>     <td>Teoma</td>     <td>44</td>     <td>2007-03-25 01:08:27</td>   </tr> </tbody></table> <p>We see that MSNbot, Gigabot and Teoma might crawl some pages with an old cached robots.txt (if you don't update it frequently).</p> <p><b style="font-size: 16px;"><a name="conclusions">Conclusions</a></b></p> <ul>   <li>do not allow search engines to index generated pages that have no value to them</li>   <li>do not allow search engines to index filter pages</li>   <li>do not allow search engines to index search results</li>   <li>&quot;Content is the king.&quot;</li>   <li>watch closely your access log on your web server (check 404 errors and redirect them)</li>   <li>use tools to check your robots.txt such as <a href="http://www.google.com/support/webmasters/bin/answer.py?answer=35237">Google's robots.txt analysis tool</a> (this tool does not apply globaly on all search engines)</li>   <li>make sure you understand the <a href="http://www.robotstxt.org/wc/robots.html">robots.txt standards</a></li> </ul> <p><b style="font-size: 16px;"><a name="solutions">Solutions</a></b></p> <p style="color: blue;">&bull; MSNbot</p> <p><span style="color: red;">[UPDATE 28 mar 07 #1]</span> - Brent Hands (PM at MSNbot Live Search's crawler) - &quot;<em>The problem here comes from the fact that we do not interpret   the &ldquo;*&rdquo; and &ldquo;$&rdquo; operators as a matter of course.&nbsp; However, we do recognize the   following pattern: </em></p> <p>Disallow:   /*.pdf$</p> <p><em>So, by adding the &ldquo;.&rdquo;, you can effectively block all documents with a specific   extension.</em>&quot;<br /> Maybe i've exagerated a bit with mod_rewrite, take this example: http://www.itpromo.net/servers/1/pdf - a PDF version of the first page from 'Servers Deals' section (http://www.itpromo.net/servers or http://www.itpromo.net/servers/1)</p> <p><span style="color: red;">[UPDATE 03 apr 07 #1]</span> - Brent Hands with the official Microsoft response:</p> <p>&quot;<em>Unfortunately, we currently only   interpret &lsquo;*&rsquo; when used to filter by file extension. &nbsp;As   such, the following will work:</em></p> <p>User-agent: * <br />   Disallow: /*.pdf$ <br />   Disallow: /*.xls$</p> <p><em>But this is the only context in   which MSNBot understands wildcards. [...] we will work resolve this as quickly as possible</em>.&quot;</p> <p style="color: blue;">&bull; Ask.com / Teoma</p> <p><span style="color: red;">[UPDATE 28 mar 07 #2]</span> - Ask.com responded very quick &quot;<em>We have forwarded this situation to the appropriate technical team for   investigation, and will get back to you as soon as we are able. Thank you for   your patience.</em>&quot;.</p> <p><span style="color: red;">[UPDATE 03 apr 07 #2]</span> - Official Ask.com response: &quot;<em>We do support the '*' wildcard character, but unfortunately, not regular   expressions.</em>&quot; I'm still having problems blocking Teoma from crawling URLs like: <a rel="nofollow" target="_blank" href="http://www.itpromo.net/pda/full/xls">/pda/full/xls</a> or <a rel="nofollow" target="_blank" href="http://www.itpromo.net/notebook/full/html">/notebook/full/html</a></p>