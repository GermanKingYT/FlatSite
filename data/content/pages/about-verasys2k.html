<h5><a name="about2k">About the project</a></h5> <p>The project started as my interest for database mining growed. I was always troubled about how Google, Yahoo and other major search engines gather and show the results for a certain query. Every database developer knows that finding and returning the right results is more than a simple regex query. The results MUST be relevat for the user.</p> <p>In order to do that you have to think about certain aspects before start developing your extractor script:</p> <ul>   <li>what is the size of the database (scalability issues)</li>   <li>what type of data rows are we searching (important for the way we rank the results)</li>   <li>are there any extra sorting measures that we have to bear in mind (maybe you find two pages from the same url that rank the same, and mabe you want to show them both &lt;&lt;Google style&gt;&gt; as they may be relevant to the user)</li> </ul> <p>At first i knew i could never match the power of a real search engine, so i started it small. I gathered about 400+ .ro sites that have the same topic: Tourism and travel . The reason why i chose this topic is because i knew i will find medium-sized web pages varying from:</p> <ul>   <li>small (1-20 unique pages) to medium-sized sites (about 15.000+ unique pages)</li>   <li>pages with bad HTML code writen and pages that we full CSS coded</li>   <li>the URLs of these set of pages would also vary from ones filled with variables and session ids to nice and smooth custom made URLs</li>   <li>also the last reason that i had in mind at the time was the kinds of HTTP header redirects, javascript redirects, etc.</li> </ul> <p>As the time passed, i went into a lot of trouble dealing with:</p> <ul>   <li>creating the right sitemap, gathering all the internal links from the site</li>   <li>all sorts of redirects: HTTP header, javascript, meta also frames were a problem</li>   <li>sloppy code, not everyone knows how to make a decent page with a decent code (not every page has an &lt;html&gt; tag at the beginning or at the end; same thing apply to common sense tags like &lt;head&gt;, &lt;title&gt;, &lt;body&gt;)</li>   <li>ugly URLs, that passed al sorts of characters and garbage variables, not to mention session id-s, realised that not every page has a standard extension (eg. .htm, .html, .php)</li>   <li>timeout and networking problems (still having those, because i am quite limited at timeout management and networking is not quite my field)</li>   <li>diversity of languages, charsets</li>   <li>html entities and custom HTML tags</li>   <li>stripping the tags</li>   <li>database growth</li>   <li>data processing, which is hardware consumming (regex segmentation faults, etc)</li> </ul> <p>I ran the project online during 3 months until november 2005, and in this time other problems appeared:</p> <ul>   <li>database design (transition from old concepts to new concepts takes time if you are dealing with a lot of data)</li>   <li>result caching (i had to cache the results for the queries, because the database could not deal with an instant URL -&gt; PAGE -&gt; WORD ranking and ordering)</li>   <li>slow results for multiple terms in a query (ranking takes more computation)</li>   <li>certain terms reveiled unwanted results due to some spamming techniques</li>   <li>refreshing and updating the database</li> </ul> <p>The reason why the project is temporarly offline is because of the research time that i have to spend in order to get things going. I realised that a crawler is mainly made of 3 components:</p> <ol>   <li>the fetcher (grabs HTML documents and helps in creating the sitemap)</li>   <li>the processer (grabs the locally HTML files and computes the ranks for each word in every page)</li>   <li>the searcher (this process is dealing with queries and returning results)</li> </ol> <p>As you can see, all 3 major procesess are very complex, each has it's own problems and challenges. Of course, some of you who work in the search engine industry might say: &quot;what about the spam cleaner, the duplicate cleaner ... &quot; or other intermediate process that might exist between the processer and the searcher; i did not mentioned that because i will try to discuss that later when i have some practical examples and some experience involving that processes.</p> <p>All the aspects shown above are from the situations encountered with VERASYS 2k crawler from mid 2005 until february 2006. At that point VERASYS 2k was and still is a project that crawls certain web pages and analyse them.</p> <p>In the beginning of the new year (2006), came another challenge for the 2k engine. As we progressed in the code with iT PROMO (biggest Romanian IT&amp;C e-commerce shop), i knew i had to find a way to deliver visitors a fast and reliable way of finding the desired product on site. The only solution, besides an easy navigational scheme, was to put a search engine that would find products, and find them good.</p> <p>This is when i split the project in 2 parts, and i used the theory from the first VERASYS 2k engine. Not only the theory, but the powerful concept that drives every major search engine: &quot;Find <i>fast</i> and <i>relevant results</i> for the query input&quot;. Finding <i>fast</i> the results would not be a major problem, because we are talking about a database with 90.000+ records. So we have a half static / half dinamic database: structure remains the same all the time and 20% the records are changing/updating daily.</p> <p>The other second component, and major problem, is returning the <i>relevant results</i> for the query input. Now that's a real challenge! I took a quick look at other e-commerce shops, could not find any refferences or similar examples (something to build upoan), so i knew at least i'm the first who attempts this (i guess). The final result (and yes, i still got some improvement to do) can be seen at iT PROMO section '<a href="http://www.itpromo.ro/search.aspx">Cauta in site</a>' (that's Search the site in english).</p> <p>The features that this little 2k search has are:</p> <ul>   <li>all sorts of advanced search (any-all-exact, categories search refine, price range, etc)</li>   <li>real time ranking</li>   <li>real time correction and spelling (not all therms covered yet), synonims</li>   <li>sugestions</li>   <li>multiple therm search</li> </ul> <h5><a name="more2k">More information on VERASYS 2k crawler</a></h5> <p>The crawler is made using:</p> <ul>   <li>PHP</li>   <li>MySQL</li>   <li>XML</li> </ul> <p>PHP and MySQL can both sound like &quot;limitations&quot; to you, but i have developed 2k based on these software packages because they are a part of my daily job. I guess the only thing without fronteers and fancy might be the XML part, but don't think too far, because i've used only basic XML.</p> <p>In the present i use VERASYS 2k for research like statistics about HTML tags and attributes on a webpage.</p> <h5><a name="todo2k">TODO</a></h5> <p>Any feedback to my e-mail address would be highly apreciated!</p> <ul>   <li>create a database structure with HTML tags (+deprecated) and attributes, and link them toghether</li>   <li>create the script that parse already saved HTML in order to extract the statisticts about PAGE -&gt; TAG -&gt; ATTRIBUTE</li>   <li>create a method of learning &quot;new&quot; attributes</li>   <li>finding a way to parse HTML documents using a DTD (Document Type Definition) files</li>   <li>finding a way to parse and validate HTML document just like <a href="http://validator.w3.org/">W3 Validator</a> does</li> </ul>